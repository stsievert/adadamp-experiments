{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4a57818-9737-499e-b1c7-a0cce27685e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9bcb661-64dc-4786-aa2a-defbc290a7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0059, -0.0132, -0.0145,  ..., -0.0162,  0.0236, -0.0113],\n",
      "        [ 0.0151,  0.0065, -0.0165,  ..., -0.0042,  0.0338,  0.0058],\n",
      "        [ 0.0152,  0.0002,  0.0118,  ...,  0.0038, -0.0085,  0.0549],\n",
      "        ...,\n",
      "        [-0.0196, -0.0074,  0.0205,  ...,  0.0014,  0.0389,  0.0177],\n",
      "        [-0.0530,  0.0012,  0.0115,  ...,  0.0089, -0.0431,  0.0050],\n",
      "        [ 0.0111,  0.0010, -0.0036,  ...,  0.0104,  0.0021,  0.0117]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], requires_grad=True)\n",
      "torch.Size([768])\n",
      "tensor(0.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Let's change the model to get rid of the last layers\n",
    "with torch.no_grad():\n",
    "    print(model.pooler.dense.weight)\n",
    "    eye = torch.from_numpy(np.eye(768).astype(\"float32\"))\n",
    "    model.pooler.dense.weight.data = eye\n",
    "    print(model.pooler.dense.weight)\n",
    "    \n",
    "    print(model.pooler.dense.bias.shape)\n",
    "    print(model.pooler.dense.bias.abs().sum())\n",
    "    model.pooler.dense.bias *= 0\n",
    "    print(model.pooler.dense.bias.abs().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a283fcb-2d10-4d7d-babe-fba753c9a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "images = [Image.open(requests.get(url, stream=True).raw) for _ in range(2)]\n",
    "\n",
    "DS_DIR = Path(\".\").absolute() / \"dataset\"\n",
    "images = {int(f.name.replace(\".jpg\", \"\")): f for f in (DS_DIR / \"cropped\").glob(\"*.jpg\")}\n",
    "label_train = json.loads(\n",
    "    (DS_DIR / \"labels_train.json\").read_text()\n",
    ")\n",
    "label_test = json.loads(\n",
    "    (DS_DIR / \"labels_test.json\").read_text()\n",
    ")\n",
    "label_test = {int(k): v for k, v in label_test.items()}\n",
    "label_train = {int(k): v for k, v in label_train.items()}\n",
    "\n",
    "assert len(images) == len(label_train) + len(label_test) == 10_000\n",
    "uniq_labels = set(label_test.values()).union(label_train.values())\n",
    "label_map = {label: i for i, label in enumerate(uniq_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1669ebc-446a-4224-839f-a187a2ea1460",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "def get_features(\n",
    "    labels: Dict[int, str],\n",
    "    label_map: Dict[str, int],\n",
    "    images: Dict[int, Path],\n",
    "    B=32\n",
    "):\n",
    "    # reorganize dataset\n",
    "    ids = list(sorted(labels.keys()))\n",
    "    images = [images[k] for k in ids]\n",
    "    labels = [labels[k] for k in ids]\n",
    "    \n",
    "    y = [label_map[l] for l in labels]\n",
    "\n",
    "    X = []\n",
    "    for k in itertools.count():\n",
    "        print(k)\n",
    "        img_files = images[B * k : B * (k +1)]\n",
    "        imgs = [Image.open(f) for f in img_files]\n",
    "        if not len(imgs):\n",
    "            break\n",
    "        inputs = processor(images=imgs, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        X.append(outputs[\"pooler_output\"])\n",
    "    return torch.concat(X), torch.from_numpy(np.array(y, dtype=\"int\"))\n",
    "\n",
    "X_test, y_test = get_features(label_test, label_map, images)\n",
    "X_train, y_train = get_features(label_train, label_map, images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f0d37729-31a9-4b86-bacd-28d8bebfbb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_test) == len(y_test)\n",
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_train) + len(X_test) == 10_000\n",
    "\n",
    "with open(DS_DIR / \"embedding.pt\", \"wb\") as f:\n",
    "    torch.save({\"X_test\": X_test, \"y_test\": y_test, \"X_train\": X_train, \"y_train\": y_train}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9d311fbc-0370-41eb-a6ce-6b33b3f88e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 6667)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddccc6a9-6147-4be4-8a67-b1986792d084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:finetune]",
   "language": "python",
   "name": "conda-env-finetune-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
