{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a57818-9737-499e-b1c7-a0cce27685e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bcb661-64dc-4786-aa2a-defbc290a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Let's change the model to get rid of the last layers\n",
    "with torch.no_grad():\n",
    "    print(model.pooler.dense.weight)\n",
    "    eye = torch.from_numpy(np.eye(768).astype(\"float32\"))\n",
    "    model.pooler.dense.weight.data = eye\n",
    "    print(model.pooler.dense.weight)\n",
    "    \n",
    "    print(model.pooler.dense.bias.shape)\n",
    "    print(model.pooler.dense.bias.abs().sum())\n",
    "    model.pooler.dense.bias *= 0\n",
    "    print(model.pooler.dense.bias.abs().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a283fcb-2d10-4d7d-babe-fba753c9a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "images = [Image.open(requests.get(url, stream=True).raw) for _ in range(2)]\n",
    "\n",
    "DS_DIR = Path(\".\").absolute() / \"dataset\"\n",
    "images = {int(f.name.replace(\".jpg\", \"\")): f for f in (DS_DIR / \"cropped\").glob(\"*.jpg\")}\n",
    "label_train = json.loads(\n",
    "    (DS_DIR / \"labels_train.json\").read_text()\n",
    ")\n",
    "label_test = json.loads(\n",
    "    (DS_DIR / \"labels_test.json\").read_text()\n",
    ")\n",
    "label_test = {int(k): v for k, v in label_test.items()}\n",
    "label_train = {int(k): v for k, v in label_train.items()}\n",
    "\n",
    "assert len(images) == len(label_train) + len(label_test) == 10_000\n",
    "uniq_labels = set(label_test.values()).union(label_train.values())\n",
    "label_map = {label: i for i, label in enumerate(uniq_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1669ebc-446a-4224-839f-a187a2ea1460",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_features(\n",
    "    labels: Dict[int, str],\n",
    "    label_map: Dict[str, int],\n",
    "    images: Dict[int, Path],\n",
    "    B=32\n",
    "):\n",
    "    # reorganize dataset\n",
    "    ids = list(sorted(labels.keys()))\n",
    "    images = [images[k] for k in ids]\n",
    "    labels = [labels[k] for k in ids]\n",
    "    \n",
    "    y = [label_map[l] for l in labels]\n",
    "\n",
    "    X = []\n",
    "    for k in itertools.count():\n",
    "        print(k, flush=True)\n",
    "        img_files = images[B * k : B * (k +1)]\n",
    "        imgs = [Image.open(f) for f in img_files]\n",
    "        if not len(imgs):\n",
    "            break\n",
    "        inputs = processor(images=imgs, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        X.append(outputs[\"pooler_output\"])\n",
    "    return torch.concat(X), torch.from_numpy(np.array(y, dtype=\"int\"))\n",
    "\n",
    "X_test, y_test = get_features(label_test, label_map, images)\n",
    "X_train, y_train = get_features(label_train, label_map, images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d37729-31a9-4b86-bacd-28d8bebfbb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_test) == len(y_test)\n",
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_train) + len(X_test) == 10_000\n",
    "\n",
    "with open(DS_DIR / \"embedding.pt\", \"wb\") as f:\n",
    "    torch.save({\"X_test\": X_test, \"y_test\": y_test, \"X_train\": X_train, \"y_train\": y_train}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddccc6a9-6147-4be4-8a67-b1986792d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DS_DIR / \"embedding.pt\", \"rb\") as f:\n",
    "    data = torch.load(f)\n",
    "{k: v.shape for k, v in  data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f1ba0-dab0-4f8b-b52e-3936b8f6e2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:finetune]",
   "language": "python",
   "name": "conda-env-finetune-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
